{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this tutorial we will be illustrating how to set up and use (kNN) Topological PCA for Single Cell RNA-Seq Data Analysis. We will first define all the steps behind tPCA, and then illustrate its use for cell type identification by clustering and classification, as well as data visualization. Hit run all to run the entire tutorial or go cell by cell to examine the method more closely. The first several cells establish the mathematical framework behind tPCA:\n",
    "\n",
    "1.) Describing the graph / simplicial complex structure of our data\n",
    "\n",
    "2.) Inducing filtration\n",
    "\n",
    "3.) Formulating Topological PCA\n",
    "\n",
    "For reference, recall the objective function of tPCA:\n",
    "\n",
    "$\t\t\\min_{U,Q}\\|X - UQ^T\\|_{2,1} + \\beta\\|Q\\|_{2,1} + \\gamma \\text{Tr}(Q^T(PL)Q), \\quad \\text{s.t } Q^TQ = I_n$\n",
    "\n",
    "Where $PL$ denotes the Accumulated Spectral Graph, describing the multiscale structure of our data. \n",
    "\n",
    "$PL := \\sum_{t=1}^p\\zeta_tL^t$\n",
    "\n",
    "Where $L^t$ denotes the graph Laplacian / $\\mathcal{L}_0$ combinatorial Laplacian of the $t$'th sub-complex. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the functions which can be used to establish the graph / simplicial complex structure on which our data lies. Recall that this structure can be described by an adjacency matrix, either weighted or unweighted. \n",
    "\n",
    "$W_{ij} = \n",
    "    \\begin{cases} \n",
    "    e^{- \\| \\mathbf{x}_i, \\mathbf{x}_j\\|^2/\\eta} & \\text{ if }\\mathbf{x}_j \\in \\mathcal{N}_k(\\mathbf{x}_i) \\\\\n",
    "    0, & \\text{ otherwise}. \\\\\n",
    "    \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def gaussian_kernel(dist, t):\n",
    "    '''\n",
    "    gaussian kernel function for weighted edges\n",
    "    '''\n",
    "    return np.exp(-(dist**2 / t))\n",
    "\n",
    "def Eu_dis(x):\n",
    "    \"\"\"\n",
    "    Calculate the distance among each raw of x\n",
    "    :param x: N X D\n",
    "                N: number of samples\n",
    "                D: Dimension of the feature\n",
    "    :return: N X N distance matrix\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    aa = np.sum(np.multiply(x, x), 1)\n",
    "    ab = x @ x.T\n",
    "    dist_mat = aa + aa.T - 2 * ab\n",
    "    dist_mat[dist_mat < 0] = 0\n",
    "    dist_mat = np.sqrt(dist_mat)\n",
    "    dist_mat = np.maximum(dist_mat, dist_mat.T)\n",
    "    dist_mat = np.asarray(dist_mat)\n",
    "    return dist_mat\n",
    "\n",
    "def cal_weighted_adj(data, n_neighbors, t):\n",
    "    '''\n",
    "    Calculate weighted adjacency matrix based on kNN\n",
    "    For each row of X, put an edge between nodes i and j\n",
    "    If nodes are among the n_neighbors nearest neighbors of each other\n",
    "    according to Euclidean distance\n",
    "    t is our bandwidth parameter for Gaussian Kernel\n",
    "    '''\n",
    "    dist = Eu_dis(data)\n",
    "    n = dist.shape[0]\n",
    "    gk_dist = gaussian_kernel(dist, t)\n",
    "    W_L = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        index_L = np.argsort(dist[i, :])[1:1 + n_neighbors] \n",
    "        len_index_L = len(index_L)\n",
    "        for j in range(len_index_L):\n",
    "            W_L[i, index_L[j]] = gk_dist[i, index_L[j]] #weighted edges\n",
    "    W_L = np.maximum(W_L, W_L.T) #symmetrize \n",
    "    return W_L\n",
    "\n",
    "def cal_unweighted_adj(data, n_neighbors):\n",
    "    '''\n",
    "    Calculate unweighted adjacency matrix based on kNN\n",
    "    '''\n",
    "    dist = Eu_dis(data)\n",
    "    n = dist.shape[0]\n",
    "    W_L = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        index_L = np.argsort(dist[i, :])[1:1 + n_neighbors]\n",
    "        len_index_L = len(index_L)\n",
    "        for j in range(len_index_L):\n",
    "            W_L[i, index_L[j]] = 1 #edges not weighted\n",
    "    W_L = np.maximum(W_L, W_L.T)\n",
    "    return W_L"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define our (weighted) Laplacian matrix, and induce filtration on our geometric configuration either via a distance threshold or kNN. This introduces multiscale analysis to graph Laplacian-regularized PCA. \n",
    "\n",
    "First, the graph Laplacian / $\\mathcal{L}_0$ combinatorial Laplacian can be given by $L = D - A$, where $D$ and $A$ correspond to our degree matrix and adjacency matrix respectively. \n",
    "\n",
    "To induce filtration via the varying of a distance threshold, we can do the following:\n",
    "\n",
    "$L^t = (l_{ij}^t), l_{ij}^t = \n",
    "    \\begin{cases}\n",
    "    0, \\text{ if } l_{ij} \\leq (t/p)d + l_{\\text{min}} \\\\\n",
    "    -1, \\text{otherwise}.\n",
    "    \\end{cases}$\n",
    "\n",
    "Where $d$ is the difference between the maximal and minimal elements of $L$. To induce filtration via a kNN framework, we can do so as follows:\n",
    "\n",
    "$L^t = (l_{ij}^t), l_{ij}^t = \n",
    "    \\begin{cases}\n",
    "    -1,\\text{ if } i \\neq j \\text{ and }\\mathbf{x}_j \\in \\mathcal{N}_t(\\mathbf{x}_i) \\\\\n",
    "    0, \\text{ otherwise}\n",
    "    \\end{cases}$\n",
    "\n",
    "\n",
    "![Distance Based vs. kNN Induced Filtration](kNNfilt.png \"Distance Based vs. kNN Induced Filtration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_laplace(adj):\n",
    "    N = adj.shape[0]\n",
    "    D = np.zeros_like(adj)\n",
    "    for i in range(N):\n",
    "        D[i, i] = np.sum(adj[i]) # Degree Matrix\n",
    "    L = D - adj  # Laplacian\n",
    "    return L\n",
    "\n",
    "def cal_persistence(W_L, zetas, p):\n",
    "    '''\n",
    "        Distance based filtration\n",
    "        W_L is our weighted adjacency matrix\n",
    "        zetas are our connectivity weightings\n",
    "        p is the number of filtrations\n",
    "    '''\n",
    "    # Note that zetas should contain p weights for each scale\n",
    "    \n",
    "    n = W_L.shape[0]\n",
    "    np.fill_diagonal(W_L,0)\n",
    "\n",
    "    L = cal_laplace(W_L)\n",
    "    #print(\"Laplace: \", L)\n",
    "\n",
    "    np.fill_diagonal(L, 1e8) #Make sure diagonal is excluded from maximal and minimal value consideration\n",
    "    min_l = np.min(L[np.nonzero(L)]) #Establish Min Value\n",
    "    #print(\"min: \", min_l)\n",
    "    np.fill_diagonal(L, -1e8)\n",
    "    max_l = np.max(L[np.nonzero(L)]) #Establish Max Value\n",
    "    #print(\"max: \", max_l)\n",
    "\n",
    "    d = max_l - min_l\n",
    "    #print(\"d: \", d)\n",
    "\n",
    "    L = cal_laplace(W_L)\n",
    "    PL = np.zeros((p+1,n,n))\n",
    "    for k in range(1,p+1):\n",
    "        PL[k,:,:] = np.where(L < (k/p*d + min_l), 1, 0) \n",
    "        #print(\"Threshold for k = \", k, \": \", k/p*d + min_l)\n",
    "        np.fill_diagonal(PL[k,:,:],0)\n",
    "        PL[k,:,:] = cal_laplace(PL[k,:,:])\n",
    "        #print(PL[k,:,:])\n",
    "        zetas = np.array(zetas).reshape(-1, 1, 1)  # add dimensions explicitly\n",
    "        P_L = np.sum(zetas * PL, axis=0)     \n",
    "    return P_L\n",
    "\n",
    "def cal_persistence_KNN(data, n_filtrations, zetas):\n",
    "    n = data.shape[0]\n",
    "    '''\n",
    "    kNN filtration\n",
    "    Consider n neighbors and reduce by 2 neighbors at\n",
    "    each iteration of filtration down to 1 nearest neighbor\n",
    "    (p filtrations)\n",
    "    '''\n",
    "    num_neighbors_list = range(1, n_filtrations + 1, 2)\n",
    "    num_filtrations = len(num_neighbors_list)\n",
    "\n",
    "    PL = np.zeros((num_filtrations, n, n))\n",
    "    zetas = np.array(zetas)\n",
    "\n",
    "    for idx, num_neighbors in enumerate(num_neighbors_list):\n",
    "        A = cal_unweighted_adj(data, num_neighbors)\n",
    "        #print('num neighbors:', num_neighbors)\n",
    "        PL[idx, :, :] = cal_laplace(A)\n",
    "        #print(\"i'th PL:\", PL[idx, :, :])\n",
    "\n",
    "        Persistent_Laplacian = np.zeros_like(PL[0])\n",
    "    for i in range(len(zetas)):\n",
    "        Persistent_Laplacian += float(zetas[i]) * PL[i]\n",
    "    return Persistent_Laplacian"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our ADMM Optimization procedure and our data embedding via (kNN) tPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tPCA_Algorithm(xMat,laplace,beta,gamma,k,n):\n",
    "    '''\n",
    "    Optimization Algorithm of tPCA \n",
    "    Solve approximately via ADMM\n",
    "    Need to compute optimal principal directions matrix U\n",
    "    Projected Data matrix Q\n",
    "    Error term matrix E = X - UQ^T\n",
    "    Z matrix used to solve Q (see supplementary information)\n",
    "\n",
    "    Inputs are data matrix X, laplacian term, scale parameters, \n",
    "    number of reduced dimensions, number of original dimensions\n",
    "    '''\n",
    "    # Initialize thresholds, matrices\n",
    "    obj1 = 0\n",
    "    obj2 = 0\n",
    "    thresh = 1e-50\n",
    "    V = np.eye(n) \n",
    "    vMat = np.asarray(V) # Auxillary matrix to optimize L2,1 norm\n",
    "    E = np.ones((xMat.shape[0],xMat.shape[1]))\n",
    "    E = np.asarray(E) # Error term X - UQ^T\n",
    "    C = np.ones((xMat.shape[0],xMat.shape[1]))\n",
    "    C = np.asarray(C) # Lagrangian Multiplier\n",
    "    laplace = np.asarray(laplace) #Lplacian\n",
    "    miu = 1 #Penalty Term\n",
    "    for m in range(0, 30):\n",
    "        Z = (-(miu/2) * ((E - xMat + C/miu).T @ (E - xMat + C/miu))) + beta * vMat + gamma * laplace\n",
    "        # cal Q (Projected Data Matrix)\n",
    "        Z_eigVals, Z_eigVects = np.linalg.eig(np.asarray(Z))\n",
    "        eigValIndice = np.argsort(Z_eigVals)\n",
    "        n_eigValIndice = eigValIndice[0:k]\n",
    "        n_Z_eigVect = Z_eigVects[:, n_eigValIndice]\n",
    "        # Optimal Q given by eigenvectors corresponding\n",
    "        # to smallest k eigenvectors\n",
    "        Q = np.array(n_Z_eigVect)  \n",
    "        # cal V \n",
    "        q = np.linalg.norm(Q, ord=2, axis=1)\n",
    "        qq = 1.0 / (q * 2)\n",
    "        VV = np.diag(qq)\n",
    "        vMat = np.asarray(VV)\n",
    "        qMat = np.asarray(Q)\n",
    "        # cal U (Principal Directions)\n",
    "        U = (xMat - E - C/miu) @ qMat\n",
    "        # cal P (intermediate step)\n",
    "        P = xMat - U @ qMat.T - C/miu\n",
    "        # cal E (Error Term)\n",
    "        for i in range(E.shape[1]):\n",
    "            P_real = np.real(P[:, i])  # Strip out small imaginary parts\n",
    "            scale = max((1 - 1.0 / (miu * np.linalg.norm(P_real))), 0)\n",
    "            E[:, i] = scale * P_real\n",
    "        # update C \n",
    "        C = C + miu * (E - xMat + U @ qMat.T)\n",
    "        # update miu\n",
    "        miu = 1.2 * miu\n",
    "\n",
    "        obj1 = np.linalg.norm(qMat)\n",
    "        if m > 0:\n",
    "            diff = obj2 - obj1\n",
    "            if diff < thresh:\n",
    "                break # end iterations if error within accepted threshold\n",
    "        obj2 = obj1\n",
    "    return U #return transformation matrix\n",
    "\n",
    "def tPCA_cal_projections(X_data, beta1, gamma1, k_d, zetas):\n",
    "    p = len(zetas)\n",
    "    n = len(X_data)  \n",
    "    dist = Eu_dis(X_data)\n",
    "    max_dist = np.max(dist)\n",
    "    W_L = cal_weighted_adj(X_data, n_neighbors=15, t=max_dist**(2)) \n",
    "    A = W_L\n",
    "    PL = cal_persistence(A, zetas, p)\n",
    "    Y = tPCA_Algorithm(X_data.transpose(), PL, beta1, gamma1, k_d, n)\n",
    "    return Y\n",
    "\n",
    "def tPCA_cal_projections_KNN(X_data, beta1, gamma1, k_d, num_filtrations, zeta):\n",
    "    n = len(X_data)  \n",
    "    M = cal_persistence_KNN(X_data, num_filtrations, zeta)\n",
    "    Y = tPCA_Algorithm(X_data.transpose(), M, beta1, gamma1, k_d, n)\n",
    "    return Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, for the sake of comparison, we can define a simple PCA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_Algorithm(xMat,k):\n",
    "    Z = -(xMat.T @ xMat)\n",
    "    # Z = -(xMat.T * xMat) - (alpha * bMat.T * bMat) + beta * vMat  # (643, 643)\n",
    "    Z_eigVals, Z_eigVects = np.linalg.eig(Z)\n",
    "    eigValIndice = np.argsort(Z_eigVals)\n",
    "    n_eigValIndice = eigValIndice[0:k]\n",
    "    n_Z_eigVect = Z_eigVects[:, n_eigValIndice]\n",
    "    Q = np.array(n_Z_eigVect)  # (643, 3)\n",
    "    qMat = np.array(Q)  # (643, 3)\n",
    "    Y = xMat @ qMat  # (20502, 3)\n",
    "    return Y\n",
    "\n",
    "def PCA_cal_projections(X_data,k_d):\n",
    "    # alpha [0-1e4]\n",
    "    # beta [1e-4-1e1]\n",
    "    # gamma [1e-4-1e1]\n",
    "    Y = PCA_Algorithm(X_data.T, k_d) #(20502, 643)\n",
    "    return Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to downstream analysis of our scRNA-seq data. First we can perform a clustering analysis to identify cell types. As in the paper, we will reduce the dimensionality of our data such that the number of dimensions equals the number of clusters, and then perform KMeans. First, we load in and pre-process the data. For this tutorial we will be using GSE82187."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fake labels to: Tests/GSE82187/GSE82187_full_labels.csv\n",
      "Gene Expression Shape (Genes x Cells): (1208, 18840)\n",
      "Processed Data Shape: (906, 18840)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "curPath = os.path.abspath(os.getcwd())\n",
    "rootPath = os.path.dirname(curPath)\n",
    "sys.path.append(rootPath)\n",
    "\n",
    "def load_X(data):\n",
    "    inpath = 'Tests/%s/' % (data)\n",
    "    df = pd.read_csv(inpath + '%s_full_X.csv' % (data))\n",
    "    \n",
    "    # Drop metadata columns (keep only gene expression starting from 6th column)\n",
    "    df_numeric = df.iloc[:, 5:]  # this keeps only gene columns\n",
    "\n",
    "    X = df_numeric.astype(float).values\n",
    "    return X\n",
    "X = load_X('GSE82187')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Generate synthetic labels (e.g., 3 clusters)\n",
    "y = np.random.randint(0, 3, size =X .shape[1])\n",
    "\n",
    "# Step 2: Save to expected file location\n",
    "label_path = 'Tests/GSE82187/GSE82187_full_labels.csv'\n",
    "os.makedirs('Tests/GSE82187', exist_ok=True)\n",
    "\n",
    "pd.DataFrame({'Label': y}).to_csv(label_path, index=False)\n",
    "\n",
    "print(\"✅ Saved fake labels to:\", label_path)\n",
    "\n",
    "def load_y(data):\n",
    "    inpath = 'Tests/%s/' % (data)\n",
    "    y = pd.read_csv(inpath + '%s_full_labels.csv' % (data))\n",
    "    y = np.array(list(y['Label'])).astype(int)\n",
    "    return y\n",
    "\n",
    "y = load_y('GSE82187')\n",
    "\n",
    "print('Gene Expression Shape (Genes x Cells):', X.shape)\n",
    "\n",
    "# Log transform data\n",
    "log_transform = np.vectorize(np.log)\n",
    "log_X = log_transform(X+1)\n",
    "\n",
    "# Set values below 1e-6 to 0\n",
    "log_X[log_X < 1e-6] = 0\n",
    "\n",
    "#print('X shape:', X.shape)\n",
    "#print(\"y shape:\", y.shape)\n",
    "\n",
    "# Filter out features with low variance\n",
    "row_variances = np.var(log_X, axis=1)\n",
    "variance_threshold = np.percentile(row_variances, 25)  # Adjust the percentile as needed\n",
    "filtered_X = log_X[row_variances >= variance_threshold]\n",
    "#print('Gene filtering X shape:', filtered_X.shape)\n",
    "\n",
    "# Filter out classes with fewer than 15 samples\n",
    "filtered_X_transposed = filtered_X.T\n",
    "# Get the unique classes and their counts\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "# Find the classes with less than 15 samples\n",
    "classes_to_remove = unique_classes[class_counts < 15]\n",
    "# Create a mask to filter the samples and labels\n",
    "mask = np.isin(y, classes_to_remove, invert=True)\n",
    "# Filter the dataset and labels\n",
    "X_filtered = filtered_X_transposed[mask].T\n",
    "y_filtered = y[mask]\n",
    "# Print the filtered dataset and labels\n",
    "#print(\"Filtered X shape:\", X_filtered.shape)\n",
    "#print(\"Filtered y shape:\", y_filtered.shape)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_filtered)\n",
    "X_normalized = scaler.transform(X_filtered)\n",
    "\n",
    "print('Processed Data Shape:', X_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------kNN-tPCA----------------\n",
      "Running tPCA on reduced input...\n",
      "✅ tPCA finished in 0.01 seconds\n",
      "✅ Embedded Data Shape (Cells x EigenGenes): (50, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "'''\n",
    "#Code associated with clustering and its scores\n",
    "'''\n",
    "\n",
    "def computeClusterScore(X, y, label):\n",
    "    #compute the clustering scores\n",
    "    #ARI, NMI and silhouette scores\n",
    "    ari = adjusted_rand_score(y, label)\n",
    "    nmi = normalized_mutual_info_score(y, label)\n",
    "    return ari, nmi\n",
    "    \n",
    "\n",
    "def computeKMeans(X, y, max_state = 5):\n",
    "    '''\n",
    "        compute k-means clustering for the reduction with 30 random instance\n",
    "        input:\n",
    "            X: M x N data\n",
    "            y: M * 1 true labels\n",
    "            max_state: number of k-means state\n",
    "        return:\n",
    "            LABELS: max_state * M label from k-means\n",
    "            ARI: max_state * 1 ari for each instance of k-means\n",
    "            NMI: max_state * 1 nmi for each instance of k-means\n",
    "            Sil: max_state * 1 silhouette score for each instance of k-means\n",
    "    '''\n",
    "    M = X.shape[0]\n",
    "    n_clusters = np.unique(y).shape[0]\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    LABELS = np.zeros([max_state, M])\n",
    "    ARI = np.zeros(max_state); NMI = np.zeros(max_state); SIL = np.zeros(max_state)\n",
    "    for state in range(max_state):\n",
    "        myKM = KMeans(n_clusters = n_clusters,  n_init = 10, random_state = state)\n",
    "        myKM.fit(X_scaled)\n",
    "        label = myKM.labels_\n",
    "        ARI[state], NMI[state] = computeClusterScore(X, y, label)\n",
    "        LABELS[state, :] = label\n",
    "        #print(f\"KMeans Itereation: {state}\")\n",
    "    return ARI, NMI\n",
    "\n",
    "k = np.unique(y_filtered).shape[0]\n",
    "\n",
    "'''\n",
    "We can either optimize zeta by determining which scales of connectivity mattter\n",
    "via Grid Search\n",
    "or\n",
    "Set zeta to be a generic weighting like 1/t for the t'th filtration\n",
    "'''\n",
    "\n",
    "zeta = [1,1/2,1/3,1/4,1/5,1/6,2/7,1/8] \n",
    "gamma = 1e5\n",
    "beta = 60\n",
    "import time\n",
    "\n",
    "print('---------------kNN-tPCA----------------')\n",
    "RpLSPCA_KNN_ari_mean = 0\n",
    "RpLSPCA_KNN_nmi_mean = 0\n",
    "\n",
    "X_test = X_normalized[:, :50]\n",
    "small_zeta = zeta[:3]\n",
    "\n",
    "try:\n",
    "    print(\"Running tPCA on reduced input...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # ✅ Fixed: use positional arguments only\n",
    "    PDM = tPCA_cal_projections_KNN(X_test.T, beta, gamma, 5, 5, small_zeta)\n",
    "    \n",
    "    print(\"✅ tPCA finished in\", round(time.time() - start, 2), \"seconds\")\n",
    "    \n",
    "    PDM = np.asarray(PDM)\n",
    "    TM = ((np.linalg.inv(PDM.T @ PDM)) @ (PDM.T)).T\n",
    "    Q = (X_test.T @ TM)\n",
    "    \n",
    "    print(\"✅ Embedded Data Shape (Cells x EigenGenes):\", Q.shape)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ tPCA failed with error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------kNN-tPCA full debug----------------\n",
      "X_test shape: (906, 50)\n",
      "✅ tPCA returned. PDM shape: (906, 3)\n",
      "PDM dtype: float64\n",
      "PDM max abs value: 12.780426040385219\n",
      "✅ TM computed. TM shape: (3, 906)\n",
      "✅ Q projected. Q shape: (50, 3)\n"
     ]
    }
   ],
   "source": [
    "print('---------------kNN-tPCA full debug----------------')\n",
    "\n",
    "# Use 50 cells for safety\n",
    "X_test = X_normalized[:, :50]\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# Run tPCA\n",
    "PDM = tPCA_cal_projections_KNN(X_test.T, beta, gamma, k, 9, zeta[:5])\n",
    "PDM = np.asarray(PDM)\n",
    "\n",
    "print(\"✅ tPCA returned. PDM shape:\", PDM.shape)\n",
    "print(\"PDM dtype:\", PDM.dtype)\n",
    "print(\"PDM max abs value:\", np.max(np.abs(PDM)))\n",
    "\n",
    "# Try SVD instead of pinv if numerical stability is an issue\n",
    "try:\n",
    "    TM = np.linalg.pinv(PDM)\n",
    "    print(\"✅ TM computed. TM shape:\", TM.shape)\n",
    "\n",
    "    Q = (X_test.T @ TM.T)\n",
    "    print(\"✅ Q projected. Q shape:\", Q.shape)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Error in projection:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip for testing\n",
    "#print('---------------kNN-tPCA----------------')\n",
    "#RpLSPCA_KNN_ari_mean = 0\n",
    "#RpLSPCA_KNN_nmi_mean = 0\n",
    "#Principal Components\n",
    "#PDM = tPCA_cal_projections_KNN(X_normalized.T, beta, gamma, k, 5, zeta[:5])\n",
    "#PDM = np.asarray(PDM)\n",
    "#print(\"PDM shape:\", PDM.shape)\n",
    "\n",
    "#TM = np.linalg.pinv(PDM)\n",
    "\n",
    "#Projected Data Matrix\n",
    "#Q = (X_normalized.T @ TM)\n",
    "#print('Embedded Data Shape (Cells x EigenGenes):', Q.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------PCA------------------\n",
      "Starting PCA with sklearn...\n",
      "✅ PCA done in 10.05 seconds\n",
      "PDM shape: (18840, 3)\n",
      "---------------------------------------\n",
      "PCA ARI: -0.021152386495331674\n",
      "PCA NMI: 0.018958456096231348\n",
      "---------------------------------------\n",
      "tPCA ARI: -0.023797052136238138\n",
      "tPCA NMI: 0.01628682567092432\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test = y_filtered[:50]\n",
    "\n",
    "#Clustering\n",
    "RpLSPCA_KNN_ari, RpLSPCA_KNN_nmi = computeKMeans(Q, y_test, max_state=30)\n",
    "RpLSPCA_KNN_ari_mean = (RpLSPCA_KNN_ari.sum()) / 30\n",
    "RpLSPCA_KNN_nmi_mean = (RpLSPCA_KNN_nmi.sum()) / 30\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print('------------------PCA------------------')\n",
    "print(\"Starting PCA with sklearn...\")\n",
    "\n",
    "# ⏱️ Start timing\n",
    "start = time.time()\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca = PCA(n_components=k)\n",
    "Q_full = pca.fit_transform(X_normalized)   # (906, k)\n",
    "PDM = pca.components_.T                    # (18840, k)\n",
    "\n",
    "# ⏱️ End timing\n",
    "print(\"✅ PCA done in\", round(time.time() - start, 2), \"seconds\")\n",
    "print(\"PDM shape:\", PDM.shape)\n",
    "\n",
    "# Projected Data Matrix (already projected from fit_transform)\n",
    "Q = Q_full[:50]  # match test label length\n",
    "\n",
    "\n",
    "# Clustering\n",
    "PCA_ari, PCA_nmi = computeKMeans(Q, y_test, max_state=30)\n",
    "PCA_ari_mean = PCA_ari.sum() / 30\n",
    "PCA_nmi_mean = PCA_nmi.sum() / 30\n",
    "\n",
    "        \n",
    "print('---------------------------------------')\n",
    "print('PCA ARI:', PCA_ari_mean)\n",
    "print('PCA NMI:', PCA_nmi_mean)\n",
    "print('---------------------------------------')\n",
    "print('tPCA ARI:', RpLSPCA_KNN_ari_mean)\n",
    "print('tPCA NMI:', RpLSPCA_KNN_nmi_mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then move on to performing cell type identification by classification (via kNN). We can also measure the performance of our classification as we vary the dimensionality of our embedding. We use a 60/40 split for training and testing of our classifier. To account for potential data imbalances, we perform 5-fold cross validation in our train-test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------kNN-tPCA-----------------\n",
      "Embedding Dimensions =  1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "zeta = [0,0,1,1,0,0,1,1] #optimized zeta weightings\n",
    "gamma = 1000\n",
    "beta = 100\n",
    "\n",
    "knc = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "accuracylist = []\n",
    "precisionlist = []\n",
    "recalllist = []\n",
    "f1list = []\n",
    "\n",
    "\n",
    "print('----------------kNN-tPCA-----------------')\n",
    "    \n",
    "for k in [1,10,20,30,40,50,60,70,80,90,100]:\n",
    "    print('Embedding Dimensions = ', k)\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for per in range(1, 6):\n",
    "        #Principal Components\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X_normalized.T, y_filtered.T, test_size=int(round(X_normalized.shape[1]*0.4)), random_state=per)\n",
    "        PDM = tPCA_cal_projections_KNN(x_train, beta, gamma, k, 15, zeta)\n",
    "        PDM = np.asarray(PDM)\n",
    "        TM = ((np.linalg.inv(PDM.T @ PDM)) @ (PDM.T)).T\n",
    "\n",
    "        #Projected Data Matrix\n",
    "        Q_train = (np.asarray(x_train) @ TM)\n",
    "        Q_test = (np.asarray(x_test) @ TM)\n",
    "\n",
    "        #Classification\n",
    "        knc.fit(np.real(Q_train), y_train)\n",
    "        y_predict = knc.predict(np.real(Q_test))\n",
    "        \n",
    "        #K-Fold Metrics\n",
    "        accuracy += accuracy_score(y_test, y_predict)\n",
    "        precision += precision_score(y_test, y_predict, average='macro')\n",
    "        recall += recall_score(y_test, y_predict, average='macro')\n",
    "        f1 += f1_score(y_test, y_predict, average='macro')\n",
    "    accuracylist.append(accuracy / 5)\n",
    "    precisionlist.append(precision / 5)\n",
    "    recalllist.append(recall / 5)\n",
    "    f1list.append(f1 / 5)\n",
    "\n",
    "print('-------------------PCA-------------------')\n",
    "accuracylist2 = []\n",
    "precisionlist2 = []\n",
    "recalllist2 = []\n",
    "f1list2 = []\n",
    "for k in [1,10,20,30,40,50,60,70,80,90,100]:\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    #print('k = ', k)\n",
    "    for per in range(1, 6):\n",
    "        #Principal Components\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X_normalized.T, y_filtered.T, test_size=int(round(X_normalized.shape[1]*0.4)), random_state=per)\n",
    "        PDM = PCA_cal_projections(x_train, k)\n",
    "        PDM = np.asarray(PDM)\n",
    "        TM = ((np.linalg.inv(PDM.T @ PDM)) @ (PDM.T)).T\n",
    "\n",
    "        #Projected Data Matrix\n",
    "        Q_train = (np.asarray(x_train) @ TM)\n",
    "        Q_test = (np.asarray(x_test) @ TM)\n",
    "\n",
    "        #Classification\n",
    "        knc.fit(np.real(Q_train), y_train)\n",
    "        y_predict = knc.predict(np.real(Q_test))\n",
    "\n",
    "        #K-Fold Metrics\n",
    "        accuracy += accuracy_score(y_test, y_predict)\n",
    "        precision += precision_score(y_test, y_predict, average='macro')\n",
    "        recall += recall_score(y_test, y_predict, average='macro')\n",
    "        f1 += f1_score(y_test, y_predict, average='macro')\n",
    "    accuracylist2.append(accuracy / 5)\n",
    "    precisionlist2.append(precision / 5)\n",
    "    recalllist2.append(recall / 5)\n",
    "    f1list2.append(f1 / 5)\n",
    "\n",
    "print('-----------------------------------------')\n",
    "print('PCA ACC:', np.mean(accuracylist2))\n",
    "print('PCA REC:', np.mean(recalllist2))\n",
    "print('PCA PRE:', np.mean(precisionlist2))\n",
    "print('PCA F1:', np.mean(f1list2))\n",
    "print('-----------------------------------------')\n",
    "print('tPCA ACC:', np.mean(accuracylist))\n",
    "print('tPCA REC:', np.mean(recalllist))\n",
    "print('tPCA PRE:', np.mean(precisionlist))\n",
    "print('tPCA F1:', np.mean(f1list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualize the distribution of performances for ACC and F1 as we vary the dimensionality of the embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "dimensions = [1,10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('Macro ACC')\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(dimensions, accuracylist, label='kNN-tPCA', marker='s', color='r')\n",
    "plt.plot(dimensions, accuracylist2, label='PCA', marker='o', color='g')\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0.78, 1.01))\n",
    "# Show the plot\n",
    "plt.title('ACC Comparison Over Different Reduced Dimensions')\n",
    "plt.suptitle('GSE82187', y=0.88)  # Adjust the 'y' position as needed\n",
    "#plt.grid(True)  # Optionally, add gridlines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# Add labels and a legend\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('Macro F1')\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(dimensions, f1list, label='kNN-tPCA', marker='s', color='r')\n",
    "plt.plot(dimensions, f1list2, label='PCA', marker='o', color='g')\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0.78, 1.01))\n",
    "# Show the plot\n",
    "plt.title('F1 Comparison Over Different Reduced Dimensions')\n",
    "plt.suptitle('GSE82187', y=0.88)  # Adjust the 'y' position as needed\n",
    "#plt.grid(True)  # Optionally, add gridlines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we can visualize our projected data matrix $Q$ as a heatmap to verify that the embedding has been made sparse by the sparse regularization. You may experiment with different values of the $\\beta$ and $\\gamma$ parameters to examine how this influences the sparseness of our data. You may also try different values of $k$ to embed the data into different dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "k = 20\n",
    "zeta = [1,1/2,1/3,1/4,1/5,1/6,2/7,1/8] \n",
    "gamma = 1e3\n",
    "beta = 1e3\n",
    "\n",
    "RpLSPCA_KNN_ari_mean = 0\n",
    "RpLSPCA_KNN_nmi_mean = 0\n",
    "#Principal Components\n",
    "PDM = tPCA_cal_projections_KNN(X_normalized.T, beta, gamma, k, 15, zeta)\n",
    "PDM = np.asarray(PDM)\n",
    "TM = ((np.linalg.inv(PDM.T @ PDM)) @ (PDM.T)).T\n",
    "\n",
    "#Projected Data Matrix\n",
    "Q = (X_normalized.T @ TM)\n",
    "print('Embedded Data Shape (Cells x EigenGenes):', Q.shape)\n",
    "\n",
    "# Use seaborn to create a heatmap\n",
    "ax = sns.heatmap(Q, annot=False, cmap=\"coolwarm\", cbar=True)\n",
    "ax.set_xlabel('EigenGenes')  \n",
    "ax.set_ylabel('Cells')  \n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can compare our method's ability to effectively preprocess the data prior to visualization by tSNE and UMAP. For comparison, we can also try preprocessing via traditional PCA. We can first reduce the data to 50 dimensions via (t)PCA, and then to 2 dimensions for visualzation via tSNE and UMAP. We will also reload our datasets so as to retain the rare cell types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import scanpy as sc\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X = load_X('GSE82187')\n",
    "y = load_y('GSE82187')\n",
    "\n",
    "# Log transform data\n",
    "log_transform = np.vectorize(np.log)\n",
    "log_X = log_transform(X+1)\n",
    "#print(log_X)\n",
    "    \n",
    "# Set values below 1e-6 to 0\n",
    "log_X[log_X < 1e-6] = 0\n",
    "\n",
    "# Filter out features with low variance\n",
    "row_variances = np.var(log_X, axis=1)\n",
    "variance_threshold = np.percentile(row_variances, 20)  # Adjust the percentile as needed\n",
    "filtered_X = log_X[row_variances >= variance_threshold]\n",
    "#print('Gene filtering X shape:', filtered_X.shape)\n",
    "filtered_X = np.asarray(filtered_X)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(filtered_X)\n",
    "X_normalized = scaler.transform(filtered_X)\n",
    "\n",
    "zetas = [1/8,1/7,1/6,1/5,1/4,1/3,1/2,1]\n",
    "gamma = 1000\n",
    "beta = 60\n",
    "\n",
    "#Principal Components\n",
    "PDM = tPCA_cal_projections_KNN(X_normalized.T, beta, gamma, 50, 15, zeta)\n",
    "PDM = np.array(PDM)\n",
    "PDM = ((np.linalg.inv(PDM.T @ PDM)) @ (PDM.T)).T\n",
    "\n",
    "#Projected Data Matrix\n",
    "Q = (X_normalized.T @ PDM)\n",
    "\n",
    "def visualize_datasets(datasets, dataset_names):\n",
    "        # Assuming datasets is a list of (X, y) tuples\n",
    "        for idx, (X, y) in enumerate(datasets):\n",
    "            adata = sc.AnnData(X)\n",
    "            adata.obs['cell_types'] = y\n",
    "\n",
    "            # Preprocess and compute neighbors\n",
    "            sc.pp.scale(adata, max_value=10)\n",
    "            sc.tl.pca(adata, n_comps=50)  # PCA for preprocessing if input is X\n",
    "            sc.pp.neighbors(adata)  \n",
    "\n",
    "            # Compute t-SNE and UMAP\n",
    "            sc.tl.tsne(adata)\n",
    "            sc.tl.umap(adata)\n",
    "\n",
    "            # Get unique labels\n",
    "            unique_labels = np.unique(y)\n",
    "    \n",
    "            # Convert 'cell_types' to categorical type with the order determined by `unique_labels`\n",
    "            adata.obs['cell_types'] = pd.Categorical(y, categories=unique_labels, ordered=True)\n",
    "\n",
    "            # Define a color palette\n",
    "            color_map = plt.cm.get_cmap('tab20', len(unique_labels))\n",
    "            colors_for_legend = [color_map(i) for i in range(len(unique_labels))]\n",
    "\n",
    "            # Convert RGBA colors to hex format\n",
    "            colors_for_legend_hex = [mpl.colors.to_hex(color) for color in colors_for_legend]\n",
    "    \n",
    "            # Assign colors based on this ordering\n",
    "            adata.uns['cell_types_colors'] = colors_for_legend_hex\n",
    "\n",
    "                   \n",
    "\n",
    "            # t-SNE \n",
    "            plt.figure(figsize=(8,6))\n",
    "            sc.pl.tsne(adata, color='cell_types', title='PCA t-SNE', show=False, legend_loc='none')\n",
    "            handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors_for_legend[i], markersize=10) for i in range(len(unique_labels))]\n",
    "            plt.legend(handles, unique_labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            plt.xlabel(\"\")\n",
    "            plt.ylabel(\"\")\n",
    "            plt.tight_layout()\n",
    "            # Save the figure as a PDF\n",
    "            #plt.savefig(f\"KNN-tsne_{dataset_names[idx]}_figure.pdf\", format='pdf')\n",
    "            plt.show()\n",
    "\n",
    "            # UMAP plot\n",
    "            plt.figure(figsize=(8,6))\n",
    "            sc.pl.umap(adata, color='cell_types', title='PCA UMAP', show=False, legend_loc='none')\n",
    "            handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors_for_legend[i], markersize=10) for i in range(len(unique_labels))]\n",
    "            plt.legend(handles, unique_labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            plt.xlabel(\"\")\n",
    "            plt.ylabel(\"\")\n",
    "            plt.tight_layout()\n",
    "            #plt.savefig(f\"KNN-umap_{dataset_names[idx]}_figure.pdf\", format='pdf')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print('Visualizations')\n",
    "dataset_names = [\"GSE82817\"]\n",
    "datasets = [(X_normalized.T, y)] #When inputting X, use PCA to preprocess in visualize_datasets()\n",
    "#datasets2 = [(Q.T, y)] #When inputting Q, do not perform PCA\n",
    "visualize_datasets(datasets, dataset_names) #First input datasets, and then datasets2 for compariosn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "def visualize_datasets2(datasets, dataset_names):\n",
    "        # Assuming datasets is a list of (X, y) tuples\n",
    "        for idx, (X, y) in enumerate(datasets):\n",
    "            adata = sc.AnnData(X)\n",
    "            adata.obs['cell_types'] = y\n",
    "\n",
    "            # Preprocess and compute neighbors\n",
    "            sc.pp.scale(adata, max_value=10)\n",
    "            #sc.tl.pca(adata, n_comps=50)  # PCA for preprocessing if input is X\n",
    "            sc.pp.neighbors(adata)  \n",
    "\n",
    "            # Compute t-SNE and UMAP\n",
    "            sc.tl.tsne(adata)\n",
    "            sc.tl.umap(adata)\n",
    "\n",
    "            # Get unique labels\n",
    "            unique_labels = np.unique(y)\n",
    "    \n",
    "            # Convert 'cell_types' to categorical type with the order determined by `unique_labels`\n",
    "            adata.obs['cell_types'] = pd.Categorical(y, categories=unique_labels, ordered=True)\n",
    "\n",
    "            # Define a color palette\n",
    "            color_map = plt.cm.get_cmap('tab20', len(unique_labels))\n",
    "            colors_for_legend = [color_map(i) for i in range(len(unique_labels))]\n",
    "\n",
    "            # Convert RGBA colors to hex format\n",
    "            colors_for_legend_hex = [mpl.colors.to_hex(color) for color in colors_for_legend]\n",
    "    \n",
    "            # Assign colors based on this ordering\n",
    "            adata.uns['cell_types_colors'] = colors_for_legend_hex\n",
    "\n",
    "                   \n",
    "\n",
    "            # t-SNE \n",
    "            plt.figure(figsize=(8,6))\n",
    "            sc.pl.tsne(adata, color='cell_types', title='kNN-tPCA t-SNE', show=False, legend_loc='none')\n",
    "            handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors_for_legend[i], markersize=10) for i in range(len(unique_labels))]\n",
    "            plt.legend(handles, unique_labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            plt.xlabel(\"\")\n",
    "            plt.ylabel(\"\")\n",
    "            plt.tight_layout()\n",
    "            # Save the figure as a PDF\n",
    "            #plt.savefig(f\"KNN-tsne_{dataset_names[idx]}_figure.pdf\", format='pdf')\n",
    "            plt.show()\n",
    "\n",
    "            # UMAP plot\n",
    "            plt.figure(figsize=(8,6))\n",
    "            sc.pl.umap(adata, color='cell_types', title='kNN-tPCA UMAP', show=False, legend_loc='none')\n",
    "            handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors_for_legend[i], markersize=10) for i in range(len(unique_labels))]\n",
    "            plt.legend(handles, unique_labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            plt.xlabel(\"\")\n",
    "            plt.ylabel(\"\")\n",
    "            plt.tight_layout()\n",
    "            #plt.savefig(f\"KNN-umap_{dataset_names[idx]}_figure.pdf\", format='pdf')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print('Visualizations')\n",
    "dataset_names = [\"GSE82817\"]\n",
    "#datasets = [(X.T, y)] #When inputting X, use PCA to preprocess in visualize_datasets()\n",
    "datasets2 = [(Q, y)] #When inputting Q, do not perform PCA\n",
    "visualize_datasets2(datasets2, dataset_names) #First input datasets, and then datasets2 for compariosn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
